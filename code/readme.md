

# write me a readme file for the pipeline code

```markdown
# Cryptocurrency Market Sentiment Analysis Pipeline

**Author:** Teddy Thomas  
**Institution:** CBCB, University of Maryland, College Park  
**Cluster:** CBCB HPC Cluster  
**Dataset:** [ElKulako/stocktwits-crypto](https://huggingface.co/datasets/ElKulako/stocktwits-crypto)  
**Citation:**  
M. Kulakowski and F. Frasincar, "Sentiment Classification of Cryptocurrency-Related Social Media Posts," IEEE Intelligent Systems, vol. 38, no. 4, pp. 5-9, July-Aug. 2023, doi: 10.1109/MIS.2023.3283170.

---

## ğŸš€ Project Overview

This project implements a scalable, modular pipeline for **sentiment classification** of cryptocurrency-related StockTwits posts. It leverages **lexicon-based**, **classical machine learning**, and **transformer-based** NLP models, optimized for distributed processing on the CBCB HPC cluster using Apache Spark and GPU acceleration.

**Supported Models:**
- VADER (lexicon-based)
- CountVectorizer/TF-IDF + Naive Bayes/Logistic Regression/Linear SVM (classical ML)
- Word2Vec + Logistic Regression (embeddings)
- BERT (transformer-based, fine-tuned for crypto sentiment)

---

## ğŸ“Š Problem Statement

**Objective:**  
Classify StockTwits posts related to Bitcoin, Ethereum, and Shiba Inu as:
- **Bullish** (2)
- **Bearish** (1)
- **Neutral** (0)

**Goal:**  
Compare the performance of different NLP approaches for accurate, scalable sentiment detection in a highly sentiment-driven market.

---

## ğŸ—ƒï¸ Dataset

- **Source:** StockTwits (ElKulako/stocktwits-crypto)
- **Period:** Train: Nov 2021â€“Jun 2022 | Test: Jun 2022
- **Volume:** 1.33M train posts, 83k test posts
- **Labels:** Bullish (2), Bearish (1), Neutral (0, inferred)
- **Cryptos:** BTC.X, ETH.X, SHIB.X

---

## ğŸ—ï¸ Pipeline Architecture

1. **Data Loading:** Distributed loading from Parquet files.
2. **Preprocessing:**  
   - Remove URLs, usernames, cashtags, hashtags, wallet addresses, emojis (except select ones), non-English scripts.
   - Lowercase, fix encoding, drop duplicates/short posts.
3. **Feature Engineering:**  
   - Tokenization, stopword removal.
   - TF-IDF/CountVectorizer or BERT embeddings.
4. **Model Training:**  
   - Classical ML (Logistic Regression, SVM)
   - Transformer-based (BERT with GPU acceleration)
5. **Evaluation:**  
   - Accuracy, Precision, Recall, F1-Score
6. **HPC Integration:**  
   - Spark for distributed data processing.
   - SLURM for job scheduling.
   - GPU support for transformer models.

---

## âš¡ Quickstart

### 1. Clone the Repository

```

git clone https://github.com/yourusername/crypto-sentiment-pipeline.git
cd crypto-sentiment-pipeline

```

### 2. Set Up the Environment

```

module load python/3.10 cuda/11.7 spark/3.3.0
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

```

### 3. Prepare the Data

Download and place the dataset in `/hpc/storage/crypto/` as `train.parquet` and `test.parquet`.

### 4. Run the Pipeline on CBCB HPC

Submit the job using SLURM:

```

sbatch scripts/run_hpc.sh

```

Or run locally for testing:

```

python main.py --method tfidf --model lr
python main.py --method bert --model lr

```

---

## ğŸ”§ Configuration

- **Pipeline options:**  
  - `--method`: `tfidf` or `bert`
  - `--model`: `lr` (Logistic Regression) or `svm` (Linear SVM)
  - `--hpc_config`: JSON string for Spark/HPC parameters

- **Edit** `scripts/run_hpc.sh` for cluster-specific resources (nodes, GPUs, memory).

---

## ğŸ“ˆ Results

| Model                       | Accuracy | Precision | Recall | F1-Score |
|-----------------------------|----------|-----------|--------|----------|
| VADER                       | 0.33     | 0.34      | 0.32   | 0.31     |
| Naive Bayes                 | 0.59     | 0.59      | 0.58   | 0.58     |
| Logistic Regression         | 0.63     | 0.63      | 0.62   | 0.62     |
| Linear SVM                  | 0.65     | 0.65      | 0.64   | 0.64     |
| Word2Vec + LR               | 0.51     | 0.52      | 0.51   | 0.50     |
| **BERT (Spark NLP)**        | **0.71** | **0.72**  | **0.71**| **0.71** |

---

## ğŸ§  Insights

- Lexicon-based models underperform on short, informal text.
- Classical models benefit from robust vectorization.
- Transformer-based models (BERT) yield the best accuracy.
- Spark and HPC parallelism enable scalable processing of millions of posts.

---

## ğŸ“‚ Repository Structure

```

â”œâ”€â”€ data/                \# Data files (not included)
â”œâ”€â”€ notebooks/           \# EDA and experiments
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features/        \# Feature engineering modules
â”‚   â”œâ”€â”€ models/          \# Model training scripts
â”‚   â”œâ”€â”€ processing/      \# Preprocessing utilities
â”‚   â””â”€â”€ visualization/   \# EDA tools
â”œâ”€â”€ scripts/             \# SLURM and pipeline scripts
â”œâ”€â”€ requirements.txt     \# Python dependencies
â”œâ”€â”€ main.py              \# Pipeline entry point
â””â”€â”€ README.md

```

---

## ğŸ“š References

1. M. Kulakowski and F. Frasincar, "Sentiment Classification of Cryptocurrency-Related Social Media Posts," IEEE Intelligent Systems, vol. 38, no. 4, pp. 5-9, July-Aug. 2023.
2. J. Devlin et al., â€œBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,â€ NAACL-HLT, 2019.
3. Databricks 2025. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.

---

## ğŸ™ Acknowledgments

Special thanks to the CBCB HPC team and the open-source community.

---

## ğŸ“¬ Contact

For questions or contributions, open an issue or contact Teddy Thomas at [your-email@umd.edu].

---
```

**Replace `yourusername` and `your-email@umd.edu` with your actual GitHub username and email.**
This README is structured for clarity, reproducibility, and ease of use for HPC and NLP practitioners.

